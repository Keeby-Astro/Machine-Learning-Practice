{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNKGc4G9kzVZ"
   },
   "source": [
    "# **Section 1: Theoretical Background**\n",
    "\n",
    "## Long Short-Term Memory Networks (LSTMs) in Sequence Modeling\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that are well-suited to modeling sequential data, such as time series or natural language. However, standard RNNs struggle with learning long-term dependencies due to the vanishing or exploding gradient problem. **Long Short-Term Memory networks (LSTMs)** address this issue by introducing a memory cell that can maintain information over long periods.\n",
    "\n",
    "### The LSTM Architecture\n",
    "\n",
    "An LSTM cell consists of several components that interact to decide what information to keep, write, or delete from the cell state. The key components are:\n",
    "\n",
    "- **Cell state ($C_t$)**: Represents the internal memory of the cell.\n",
    "- **Hidden state ($h_t$)**: Output of the cell that combines with the input at the next time step.\n",
    "- **Input gate ($i_t$)**: Decides which new information to store in the cell state.\n",
    "- **Forget gate ($f_t$)**: Decides which information to discard from the cell state.\n",
    "- **Output gate ($o_t$)**: Decides what information to output from the cell state.\n",
    "\n",
    "### Mathematical Formulations\n",
    "\n",
    "At each time step $t$, the LSTM performs the following computations:\n",
    "\n",
    "1. **Input Gate ($i_t$)**:\n",
    "\n",
    "   $\n",
    "   i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "   $\n",
    "\n",
    "2. **Forget Gate ($f_t$)**:\n",
    "\n",
    "   $\n",
    "   f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "   $\n",
    "\n",
    "3. **Cell Candidate ($\\tilde{C}_t$)**:\n",
    "\n",
    "   $\n",
    "   \\tilde{C}_t = \\tanh(W_C x_t + U_C h_{t-1} + b_C)\n",
    "   $\n",
    "\n",
    "4. **Cell State Update ($C_t$)**:\n",
    "\n",
    "   $\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   $\n",
    "\n",
    "5. **Output Gate ($o_t$)**:\n",
    "\n",
    "   $\n",
    "   o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "   $\n",
    "\n",
    "6. **Hidden State ($h_t$)**:\n",
    "\n",
    "   $\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   $\n",
    "\n",
    "Here, $\\sigma$ denotes the sigmoid function, $\\tanh$ is the hyperbolic tangent function, $x_t$ is the input at time $t$, and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### Step-by-Step Derivation\n",
    "\n",
    "1. **Compute Gates**: Calculate the values of the input, forget, and output gates using the current input and previous hidden state.\n",
    "2. **Update Cell State**: Modify the cell state by forgetting some information and adding new candidate information.\n",
    "3. **Compute Hidden State**: Generate the new hidden state based on the updated cell state and output gate.\n",
    "\n",
    "\n",
    "### Key Assumptions and Limitations\n",
    "\n",
    "- **Assumptions**:\n",
    "  - The sequential data has dependencies over varying time scales.\n",
    "  - The input sequences are of variable length.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Computationally intensive compared to standard RNNs.\n",
    "  - May still struggle with very long sequences.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Part-of-speech tagging, language translation, and text generation.\n",
    "- **Time Series Forecasting**: Stock prices, weather prediction.\n",
    "- **Speech Recognition**: Modeling temporal dependencies in audio signals.\n",
    "\n",
    "### Summary of Key Points\n",
    "\n",
    "- LSTMs mitigate the vanishing gradient problem in RNNs.\n",
    "- They use gates to control the flow of information.\n",
    "- Suitable for modeling long-term dependencies in sequential data.\n",
    "- Widely used in various domains requiring sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zRRceYKFkyjC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag scores: tensor([[-1.0572, -0.9974, -1.2597],\n",
      "        [-1.0553, -1.0094, -1.2466],\n",
      "        [-1.1263, -0.9667, -1.2193],\n",
      "        [-1.1229, -1.0327, -1.1437],\n",
      "        [-1.1395, -0.8838, -1.3211]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Converts a sequence of words to a tensor of indices.\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Sample training data\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# Create word-to-index and tag-to-index mappings\n",
    "word_to_ix = {}\n",
    "for sent, _ in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# Create character-to-index mapping\n",
    "char_to_ix = {}\n",
    "for word in word_to_ix.keys():\n",
    "    for char in word:\n",
    "        if char not in char_to_ix:\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "\n",
    "# Hyperparameters\n",
    "WORD_EMBEDDING_DIM = 6\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM = 6\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based POS tagger that incorporates character-level features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, hidden_dim,\n",
    "                 char_hidden_dim, vocab_size, tagset_size, char_vocab_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        # Word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "\n",
    "        # Character embeddings and LSTM\n",
    "        self.char_embeddings = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "\n",
    "        # Main LSTM\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, hidden_dim)\n",
    "\n",
    "        # Linear layer mapping to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, words):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sentence: Tensor of word indices.\n",
    "            words: List of words corresponding to the indices.\n",
    "        Returns:\n",
    "            tag_scores: Log probabilities of tags for each word.\n",
    "        \"\"\"\n",
    "        # Initialize list to hold combined embeddings\n",
    "        embeddings = []\n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            # Prepare character-level inputs\n",
    "            char_idxs = prepare_sequence(word, char_to_ix)\n",
    "            char_embeds = self.char_embeddings(char_idxs).view(len(word), 1, -1)\n",
    "            _, (char_hidden, _) = self.char_lstm(char_embeds)\n",
    "            char_hidden = char_hidden.view(-1)\n",
    "\n",
    "            # Get character embeddings and pass through char LSTM\n",
    "            char_hidden = char_hidden.view(-1)\n",
    "            # embeddings.append(char_hidden)  # Remove this line\n",
    "\n",
    "            # Get word embedding and concatenate with char-level representation\n",
    "            word_embed = self.word_embeddings(sentence[idx]).view(1, 1, -1)\n",
    "            combined = torch.cat((word_embed, char_hidden.view(1, 1, -1)), 2)\n",
    "            embeddings.append(combined.view(1, -1))\n",
    "\n",
    "        # Stack embeddings and pass through main LSTM\n",
    "        embeddings = torch.stack(embeddings).view(len(sentence), 1, -1)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMTagger(\n",
    "    WORD_EMBEDDING_DIM,\n",
    "    CHAR_EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    CHAR_HIDDEN_DIM,\n",
    "    len(word_to_ix),\n",
    "    len(tag_to_ix),\n",
    "    len(char_to_ix)\n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop (fill in the missing parts)\n",
    "for epoch in range(10):  # Reduced epochs for brevity\n",
    "    for sentence, tags in training_data:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare inputs\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Forward pass\n",
    "        tag_scores = model(inputs, sentence)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model (fill in the missing parts)\n",
    "with torch.no_grad():\n",
    "    test_sentence = training_data[0][0]\n",
    "    inputs = prepare_sequence(test_sentence, word_to_ix)\n",
    "    tag_scores = model(inputs, test_sentence)\n",
    "    print(\"Tag scores:\", tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6WEdr0AlY76"
   },
   "source": [
    "# **Section 1: Theoretical Background**\n",
    "\n",
    "## Gated Recurrent Units (GRUs) in Sequence Modeling\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are powerful for handling sequential data but suffer from the vanishing and exploding gradient problems, which hamper learning long-term dependencies. **Gated Recurrent Units (GRUs)** are a type of RNN architecture designed to mitigate these issues by introducing gating mechanisms, simplifying the architecture compared to LSTMs while retaining performance.\n",
    "\n",
    "### The GRU Architecture\n",
    "\n",
    "A GRU combines the hidden state and cell state of an LSTM into a single hidden state. It uses two gates:\n",
    "\n",
    "- **Update Gate ($z_t$)**: Determines how much of the previous hidden state to keep.\n",
    "- **Reset Gate ($r_t$)**: Determines how to combine the new input with the previous memory.\n",
    "\n",
    "### Mathematical Formulations\n",
    "\n",
    "At each time step $t$, the GRU performs the following computations:\n",
    "\n",
    "1. **Update Gate ($z_t$)**:\n",
    "\n",
    "   $\n",
    "   z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "   $\n",
    "\n",
    "2. **Reset Gate ($r_t$)**:\n",
    "\n",
    "   $\n",
    "   r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "   $\n",
    "\n",
    "3. **Candidate Activation ($\\tilde{h}_t$)**:\n",
    "\n",
    "   $\n",
    "   \\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
    "   $\n",
    "\n",
    "4. **Hidden State Update ($h_t$)**:\n",
    "\n",
    "   $\n",
    "   h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "   $\n",
    "\n",
    "Here, $\\sigma$ is the sigmoid function, $\\tanh$ is the hyperbolic tangent function, $x_t$ is the input at time $t$, $h_{t-1}$ is the previous hidden state, and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### Step-by-Step Derivation\n",
    "\n",
    "1. **Compute Update and Reset Gates**: Calculate $z_t$ and $r_t$ using the current input and previous hidden state.\n",
    "2. **Compute Candidate Activation**: Generate a candidate hidden state $\\tilde{h}_t$ using the reset gate to modulate the influence of the previous hidden state.\n",
    "3. **Update Hidden State**: Combine the previous hidden state and the candidate activation using the update gate.\n",
    "\n",
    "### Key Assumptions and Limitations\n",
    "\n",
    "- **Assumptions**:\n",
    "  - The sequential data exhibits temporal dependencies.\n",
    "  - The model benefits from gating mechanisms to control information flow.\n",
    "\n",
    "- **Limitations**:\n",
    "  - May not capture dependencies as long as those LSTMs can, due to the simpler architecture.\n",
    "  - Like all RNNs, GRUs can be computationally intensive for long sequences.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Machine translation, text summarization, and sentiment analysis.\n",
    "- **Speech Recognition**: Modeling temporal patterns in audio data.\n",
    "- **Time Series Analysis**: Forecasting and anomaly detection in financial or sensor data.\n",
    "\n",
    "### Summary of Key Points\n",
    "\n",
    "- GRUs are a simplified version of LSTMs with fewer gates.\n",
    "- They mitigate the vanishing gradient problem in standard RNNs.\n",
    "- GRUs use update and reset gates to control information flow.\n",
    "- They are computationally efficient and perform well on sequence modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RKwD2TGrlZe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag scores: tensor([[-0.8481, -1.0790, -1.4618],\n",
      "        [-0.7996, -1.1305, -1.4801],\n",
      "        [-1.0361, -1.0181, -1.2592],\n",
      "        [-1.0467, -0.9545, -1.3322],\n",
      "        [-1.0948, -1.0075, -1.2031]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Converts a sequence of words to a tensor of indices.\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Sample training data\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# Create word-to-index and tag-to-index mappings\n",
    "word_to_ix = {}\n",
    "for sent, _ in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# Create character-to-index mapping\n",
    "char_to_ix = {}\n",
    "for word in word_to_ix.keys():\n",
    "    for char in word:\n",
    "        if char not in char_to_ix:\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "\n",
    "# Hyperparameters\n",
    "WORD_EMBEDDING_DIM = 6\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM = 6\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "\n",
    "class GRUTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based POS tagger that incorporates character-level features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, hidden_dim,\n",
    "                 char_hidden_dim, vocab_size, tagset_size, char_vocab_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        # Word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "\n",
    "        # Character embeddings and GRU\n",
    "        self.char_embeddings = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_gru = nn.GRU(char_embedding_dim, char_hidden_dim)\n",
    "\n",
    "        # Main GRU\n",
    "        self.gru = nn.GRU(word_embedding_dim + char_hidden_dim, hidden_dim)\n",
    "\n",
    "        # Linear layer mapping to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, words):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sentence: Tensor of word indices.\n",
    "            words: List of words corresponding to the indices.\n",
    "        Returns:\n",
    "            tag_scores: Log probabilities of tags for each word.\n",
    "        \"\"\"\n",
    "        # Initialize list to hold combined embeddings\n",
    "        embeddings = []\n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            # Prepare character-level inputs\n",
    "            # Convert each character to its index and create a tensor\n",
    "            char_idxs = prepare_sequence(word, char_to_ix)\n",
    "            char_embeds = self.char_embeddings(char_idxs).view(len(word), 1, -1)\n",
    "\n",
    "            # Get character embeddings and pass through char GRU\n",
    "            # Obtain character embeddings\n",
    "            _, char_hidden = self.char_gru(char_embeds)\n",
    "\n",
    "            # Get word embedding and concatenate with char-level representation\n",
    "            # Obtain word embedding\n",
    "            word_embed = self.word_embeddings(sentence[idx])\n",
    "\n",
    "            # Concatenate word embedding with char-level hidden state\n",
    "            combined = torch.cat((word_embed, char_hidden.view(-1)), dim=0)\n",
    "            embeddings.append(combined)\n",
    "\n",
    "        # Stack embeddings and pass through main GRU\n",
    "        embeddings = torch.stack(embeddings).view(len(sentence), 1, -1)\n",
    "        gru_out, _ = self.gru(embeddings)\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GRUTagger(\n",
    "    WORD_EMBEDDING_DIM,\n",
    "    CHAR_EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    CHAR_HIDDEN_DIM,\n",
    "    len(word_to_ix),\n",
    "    len(tag_to_ix),\n",
    "    len(char_to_ix)\n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop (fill in the missing parts)\n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    for sentence, tags in training_data:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare inputs\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Forward pass\n",
    "        tag_scores = model(inputs, sentence)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model (fill in the missing parts)\n",
    "with torch.no_grad():\n",
    "    test_sentence = training_data[0][0]\n",
    "    inputs = prepare_sequence(test_sentence, word_to_ix)\n",
    "    tag_scores = model(inputs, test_sentence)\n",
    "    print(\"Tag scores:\", tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V743LLrmOcR"
   },
   "source": [
    "# **Section 1: Theoretical Background**\n",
    "\n",
    "## Recurrent Neural Networks (RNNs) and Teacher Forcing in Sequence Modeling\n",
    "\n",
    "Sequential data, such as time series and natural language, require models that can capture temporal dependencies. **Recurrent Neural Networks (RNNs)** are a class of neural networks designed for this purpose. However, training RNNs can be challenging due to issues like vanishing gradients and the complexities involved in sequence generation. **Teacher Forcing** is a training strategy used to address some of these challenges by providing the model with the ground truth output from the previous time step during training.\n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "An RNN processes sequences by maintaining a hidden state that is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "At each time step $ t $:\n",
    "\n",
    "1. **Hidden State Update**:\n",
    "\n",
    "   $\n",
    "   h_t = \\tanh(W_{hx} x_t + W_{hh} h_{t-1} + b_h)\n",
    "   $\n",
    "\n",
    "   - $ x_t $: Input at time $ t $\n",
    "   - $ h_{t-1} $: Hidden state from the previous time step\n",
    "   - $ W_{hx}, W_{hh} $: Weight matrices\n",
    "   - $ b_h $: Bias vector\n",
    "\n",
    "2. **Output**:\n",
    "\n",
    "   $\n",
    "   y_t = W_{hy} h_t + b_y\n",
    "   $\n",
    "\n",
    "   - $ y_t $: Output at time $ t $\n",
    "   - $ W_{hy} $: Output weight matrix\n",
    "   - $ b_y $: Output bias\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Memory of Previous Inputs**: The hidden state $ h_t $ serves as a memory of previous inputs.\n",
    "- **Shared Parameters**: The weights are shared across time steps.\n",
    "- **Challenges**: Standard RNNs struggle with long-term dependencies due to vanishing or exploding gradients.\n",
    "\n",
    "### Teacher Forcing\n",
    "\n",
    "Teacher forcing is a training strategy where, during training, the model receives the actual ground truth output from the previous time step as input, instead of its own previous output.\n",
    "\n",
    "#### Mechanism\n",
    "\n",
    "- **With Teacher Forcing**:\n",
    "\n",
    "  At each time step, the ground truth $ y_{t-1} $ is used as input for generating $ y_t $.\n",
    "\n",
    "- **Without Teacher Forcing**:\n",
    "\n",
    "  The model uses its own predicted output $ \\hat{y}_{t-1} $ from the previous time step to generate $ y_t $.\n",
    "\n",
    "#### Mathematical Representation\n",
    "\n",
    "- **With Teacher Forcing**:\n",
    "\n",
    "  $\n",
    "  h_t = \\tanh(W_{hx} x_t + W_{hy} y_{t-1} + W_{hh} h_{t-1} + b_h)\n",
    "  $\n",
    "\n",
    "- **Without Teacher Forcing**:\n",
    "\n",
    "  $\n",
    "  h_t = \\tanh(W_{hx} x_t + W_{hy} \\hat{y}_{t-1} + W_{hh} h_{t-1} + b_h)\n",
    "  $\n",
    "\n",
    "### Step-by-Step Derivation with Teacher Forcing\n",
    "\n",
    "1. **Initialization**: Start with an initial hidden state $ h_0 $ (often set to zeros).\n",
    "2. **For each time step $ t $**:\n",
    "   - Compute the hidden state $ h_t $ using the ground truth output $ y_{t-1} $.\n",
    "   - Generate the output $ y_t $ based on $ h_t $.\n",
    "\n",
    "### Contrasting with and without Teacher Forcing\n",
    "\n",
    "- **Convergence**:\n",
    "\n",
    "  - *With Teacher Forcing*: The model typically converges faster because it receives correct context from the ground truth.\n",
    "  - *Without Teacher Forcing*: Training may be slower and less stable due to accumulated errors from previous predictions.\n",
    "\n",
    "- **Exposure Bias**:\n",
    "\n",
    "  - *With Teacher Forcing*: The model may not learn to recover from its own mistakes because it rarely encounters them during training.\n",
    "  - *Without Teacher Forcing*: The model learns to handle its own errors, potentially improving robustness.\n",
    "\n",
    "### Key Assumptions and Limitations\n",
    "\n",
    "- **Assumptions**:\n",
    "\n",
    "  - The ground truth sequence is available during training.\n",
    "  - Sequential dependencies are important for the task.\n",
    "\n",
    "- **Limitations**:\n",
    "\n",
    "  - *Teacher Forcing* can lead to discrepancies between training and inference (known as exposure bias).\n",
    "  - Without teacher forcing, training may be less efficient due to error accumulation.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Language Modeling**: Predicting the next word in a sentence.\n",
    "- **Machine Translation**: Translating sequences from one language to another.\n",
    "- **Speech Recognition**: Transcribing audio sequences into text.\n",
    "\n",
    "### Summary of Key Points\n",
    "\n",
    "- RNNs are suitable for modeling sequential data but face challenges with long-term dependencies.\n",
    "- Teacher forcing is a strategy to stabilize and accelerate training by using ground truth outputs.\n",
    "- There is a trade-off between training efficiency and robustness to errors when using teacher forcing.\n",
    "- Understanding the effects of teacher forcing is crucial for designing effective sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0oPmdelXmPEp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag scores with teacher forcing: tensor([[-0.6665, -1.0373, -2.0242],\n",
      "        [-1.5274, -0.5120, -1.6948],\n",
      "        [-2.0098, -1.0183, -0.6836],\n",
      "        [-0.5246, -1.3591, -1.8886],\n",
      "        [-1.5782, -0.5187, -1.6176]])\n",
      "Tag scores without teacher forcing: tensor([[-0.6665, -1.0373, -2.0242],\n",
      "        [-1.5320, -0.5137, -1.6839],\n",
      "        [-1.9086, -0.9977, -0.7278],\n",
      "        [-0.6094, -1.3147, -1.6726],\n",
      "        [-1.4744, -0.5879, -1.5343]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Converts a sequence of words or tags to a tensor of indices.\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Sample training data\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# Create word-to-index and tag-to-index mappings\n",
    "word_to_ix = {}\n",
    "for sent, _ in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "class RNNTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based POS tagger with optional teacher forcing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        # Word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim + tagset_size, hidden_dim)\n",
    "\n",
    "        # Linear layer mapping to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, targets=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sentence: Tensor of word indices.\n",
    "            targets: Tensor of tag indices (ground truth).\n",
    "            teacher_forcing: Boolean indicating whether to use teacher forcing.\n",
    "        Returns:\n",
    "            tag_scores: Log probabilities of tags for each word.\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        tag_scores = []\n",
    "        hidden = torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "        # Initialize previous output\n",
    "        prev_output = torch.zeros(1, self.tagset_size)\n",
    "\n",
    "        for i in range(len(sentence)):\n",
    "            # Determine input for current time step\n",
    "            if teacher_forcing and targets is not None and i > 0:\n",
    "                # Use ground truth tag from previous time step\n",
    "                prev_tag = torch.zeros(1, self.tagset_size)\n",
    "                prev_tag[0][targets[i - 1]] = 1\n",
    "                rnn_input = torch.cat((embeds[i].view(1, -1), prev_tag), dim=1)\n",
    "            else:\n",
    "                # Use previous prediction\n",
    "                rnn_input = torch.cat((embeds[i].view(1, -1), prev_output), dim=1)\n",
    "\n",
    "            # TODO: Pass through RNN\n",
    "            rnn_output, hidden = self.rnn(rnn_input.view(1, 1, -1), hidden)\n",
    "\n",
    "            # TODO: Compute tag scores\n",
    "            tag_space = self.hidden2tag(rnn_output.view(1, -1))\n",
    "            tag_prob = F.log_softmax(tag_space, dim=1)\n",
    "            tag_scores.append(tag_prob)\n",
    "\n",
    "            # Update previous output\n",
    "            if teacher_forcing and targets is not None:\n",
    "                prev_output = torch.zeros(1, self.tagset_size)\n",
    "                prev_output[0][targets[i]] = 1\n",
    "            else:\n",
    "                prev_output = torch.exp(tag_prob)\n",
    "\n",
    "        tag_scores = torch.cat(tag_scores, dim=0)\n",
    "        return tag_scores\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNTagger(\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    len(word_to_ix),\n",
    "    len(tag_to_ix)\n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop (fill in the missing parts)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for sentence, tags in training_data:\n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Prepare inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Forward pass with teacher forcing\n",
    "        tag_scores = model(sentence_in, targets, teacher_forcing=True)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model with and without teacher forcing\n",
    "with torch.no_grad():\n",
    "    test_sentence = training_data[0][0]\n",
    "    inputs = prepare_sequence(test_sentence, word_to_ix)\n",
    "    targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
    "\n",
    "    # With teacher forcing\n",
    "    tag_scores_tf = model(inputs, targets, teacher_forcing=True)\n",
    "    print(\"Tag scores with teacher forcing:\", tag_scores_tf)\n",
    "\n",
    "    # Without teacher forcing\n",
    "    tag_scores_no_tf = model(inputs, teacher_forcing=False)\n",
    "    print(\"Tag scores without teacher forcing:\", tag_scores_no_tf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
