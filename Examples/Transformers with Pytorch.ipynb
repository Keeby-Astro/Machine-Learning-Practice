{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjT_uIuVAYJm"
   },
   "source": [
    "## Developing Single-Head Attention\n",
    "\n",
    "\n",
    "### Section 1: Conceptual Background\n",
    "\n",
    "#### 1.1 Theoretical Foundation and Key Concepts\n",
    "\n",
    "**Attention Mechanisms** have revolutionized the field of natural language processing and beyond by allowing models to focus on relevant parts of the input data when making predictions. **Single-Head Attention** is the foundational component of the more complex multi-head attention mechanisms used in transformer architectures.\n",
    "\n",
    "**Key Components of Single-Head Attention:**\n",
    "- **Queries (Q)**: Represent the current step's information.\n",
    "- **Keys (K)**: Represent the information to be matched against the queries.\n",
    "- **Values (V)**: Represent the information to be aggregated based on the attention scores.\n",
    "- **Attention Scores**: Calculated by measuring the compatibility between queries and keys.\n",
    "- **Weighted Sum**: Aggregates the values based on the attention scores to produce the final output.\n",
    "\n",
    "#### 1.2 Real-World Applications and Relevance\n",
    "\n",
    "Single-Head Attention is widely used in various applications, including:\n",
    "- **Machine Translation**: Enhancing the ability of models to translate sentences by focusing on relevant words.\n",
    "- **Text Summarization**: Allowing models to generate concise summaries by attending to important parts of the text.\n",
    "- **Image Captioning**: Enabling models to describe images by focusing on relevant regions.\n",
    "- **Speech Recognition**: Improving the accuracy of transcriptions by concentrating on pertinent audio segments.\n",
    "\n",
    "**Example Application**: In machine translation, single-head attention helps the model align words in the source language with their corresponding words in the target language, ensuring accurate and contextually relevant translations.\n",
    "\n",
    "#### 1.3 Prerequisite Knowledge\n",
    "\n",
    "To effectively complete this assignment, students should be familiar with:\n",
    "- **Python Programming**: Basic syntax and data structures.\n",
    "- **PyTorch**: Understanding of tensors, autograd, and neural network modules.\n",
    "- **Deep Learning Concepts**: Knowledge of neural networks, linear transformations, activation functions, and loss functions.\n",
    "- **Basic Understanding of Natural Language Processing (NLP)**: Familiarity with sequence data and tokenization.\n",
    "\n",
    "#### 1.4 Mathematical Concepts and Formulas\n",
    "\n",
    "Understanding the following mathematical concepts is essential:\n",
    "\n",
    "- **Dot-Product Attention**: Calculates the attention scores by taking the dot product of queries and keys.\n",
    "  \n",
    "  $\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $\n",
    "  \n",
    "  - $ Q $: Queries matrix\n",
    "  - $ K $: Keys matrix\n",
    "  - $ V $: Values matrix\n",
    "  - $ d_k $: Dimension of the keys\n",
    "\n",
    "- **Softmax Function**: Converts raw attention scores into probabilities that sum to one.\n",
    "\n",
    "  $\n",
    "  \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "  $\n",
    "\n",
    "- **Scaled Dot-Product**: Scaling the dot product by the square root of the key dimension to prevent large values that can destabilize the softmax.\n",
    "\n",
    "- **Linear Transformation**: Applying linear layers to project inputs into queries, keys, and values.\n",
    "\n",
    "#### 1.5 Specific Algorithms and Techniques\n",
    "\n",
    "- **Single-Head Attention Mechanism**: Implements the attention calculation using a single set of queries, keys, and values.\n",
    "- **Linear Layers**: Used to project input embeddings into queries, keys, and values.\n",
    "- **Masking (Optional)**: Prevents the model from attending to certain positions, useful in tasks like language modeling.\n",
    "- **Residual Connections and Layer Normalization (Advanced)**: Enhances training stability and performance.\n",
    "\n",
    "#### 1.6 Common Pitfalls and Misconceptions\n",
    "\n",
    "- **Incorrect Dimension Alignment**: Misaligning the dimensions of queries, keys, and values can lead to matrix multiplication errors.\n",
    "- **Ignoring Scaling Factor**: Forgetting to scale the dot product by the square root of the key dimension can cause softmax to produce extremely small gradients.\n",
    "- **Overcomplicating the Mechanism**: Single-head attention is foundational; adding unnecessary complexity can hinder understanding.\n",
    "- **Neglecting to Use Softmax Properly**: Applying softmax incorrectly can result in invalid attention distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "308fXmp4AYfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.6096616592258215, Val Accuracy = 0.747\n",
      "Epoch 2: Val Loss = 0.2306022602133453, Val Accuracy = 0.922\n",
      "Epoch 3: Val Loss = 0.02071854089444969, Val Accuracy = 0.997\n",
      "Epoch 4: Val Loss = 0.0021022752171120374, Val Accuracy = 1.0\n",
      "Epoch 5: Val Loss = 0.0006700000676573836, Val Accuracy = 1.0\n",
      "Epoch 6: Val Loss = 0.00039312213766606874, Val Accuracy = 1.0\n",
      "Epoch 7: Val Loss = 0.0002756175304057251, Val Accuracy = 1.0\n",
      "Epoch 8: Val Loss = 0.00020742487322422676, Val Accuracy = 1.0\n",
      "Epoch 9: Val Loss = 0.0001633970225611847, Val Accuracy = 1.0\n",
      "Epoch 10: Val Loss = 0.00013315497858457093, Val Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Section 2.1: Single-Head Attention Module\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # Define linear layers for queries, keys, and values\n",
    "        self.query_linear = nn.Linear(self.embed_size, self.embed_size)\n",
    "        self.key_linear = nn.Linear(self.embed_size, self.embed_size)\n",
    "        self.value_linear = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "        # Define the final linear layer\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Apply linear transformations to queries, keys, and values\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "\n",
    "        # Reshape queries, keys, and values for multi-head attention\n",
    "        queries = queries.view(N, query_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Compute the dot product attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        # Apply scaling\n",
    "        scaling_factor = np.sqrt(self.head_dim)\n",
    "        energy = energy / scaling_factor\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        # Compute the weighted sum of values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "\n",
    "        # Concatenate the heads\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)\n",
    "\n",
    "        # Pass through the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Section 2.2: Simple Model Using Single-Head Attention\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    def __init__(self, embed_size, heads, vocab_size, num_classes):\n",
    "        super(SimpleAttentionModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "\n",
    "        # Define an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Initialize the SingleHeadAttention module\n",
    "        self.attention = SingleHeadAttention(embed_size, heads)\n",
    "\n",
    "        # Define a fully connected output layer\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pass input through embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Apply single-head attention\n",
    "        attention_out = self.attention(embedded, embedded, embedded, mask)\n",
    "\n",
    "        # Pool the attention output\n",
    "        pooled = attention_out.mean(dim=1)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(pooled)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Section 2.3: Training Loop (Simplified)\n",
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        # Move data to the specified device\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Section 2.4: Evaluation Function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move data to the specified device\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Section 2.5: Main Function to Initialize and Train the Model\n",
    "def main():\n",
    "    # Set hyperparameters\n",
    "    embed_size = 256\n",
    "    heads = 1\n",
    "    vocab_size = 10000  # Example vocabulary size\n",
    "    num_classes = 2     # Example number of classes\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Initialize the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleAttentionModel(embed_size, heads, vocab_size, num_classes).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare data loaders (use dummy data for simplicity)\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    # Example dummy data\n",
    "    inputs = torch.randint(0, vocab_size, (1000, 10))  # (batch_size, sequence_length)\n",
    "    targets = torch.randint(0, num_classes, (1000,))\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Val Loss = {val_loss}, Val Accuracy = {val_acc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpK6-nWUCy-A"
   },
   "source": [
    "### Conceptual Background\n",
    "\n",
    "#### 1.1 Theoretical Foundation and Key Concepts\n",
    "\n",
    "**Attention Mechanisms** have significantly advanced the capabilities of neural networks, particularly in the field of natural language processing. **Multi-Head Attention** is an extension of the single-head attention mechanism, allowing the model to focus on different representation subspaces at different positions.\n",
    "\n",
    "**Key Components of Multi-Head Attention:**\n",
    "- **Multiple Heads**: Allows the model to attend to information from different representation subspaces.\n",
    "- **Queries (Q)**, **Keys (K)**, and **Values (V)**: Similar to single-head attention but projected separately for each head.\n",
    "- **Attention Scores**: Calculated for each head to determine the relevance of keys to the queries.\n",
    "- **Concatenation and Linear Transformation**: The outputs from all heads are concatenated and passed through a final linear layer.\n",
    "\n",
    "**Advantages of Multi-Head Attention:**\n",
    "- **Enhanced Representation**: Captures diverse aspects of the input by attending to different parts simultaneously.\n",
    "- **Improved Learning**: Facilitates better gradient flow and learning dynamics compared to single-head attention.\n",
    "\n",
    "#### 1.2 Real-World Applications and Relevance\n",
    "\n",
    "Multi-Head Attention is a cornerstone of the Transformer architecture and is widely used in various applications, including:\n",
    "- **Machine Translation**: Enhances the ability to translate sentences by capturing complex relationships between words.\n",
    "- **Text Summarization**: Allows models to generate concise summaries by focusing on different parts of the text.\n",
    "- **Question Answering**: Improves the accuracy of answering systems by attending to relevant information in the context.\n",
    "- **Image Captioning**: Enables models to describe images by focusing on various regions and their relationships.\n",
    "- **Speech Recognition**: Enhances transcription accuracy by attending to pertinent audio segments.\n",
    "\n",
    "**Example Application**: In machine translation, multi-head attention helps the model understand and translate sentences by attending to different words and their contextual relationships simultaneously, leading to more accurate and fluent translations.\n",
    "\n",
    "#### 1.3 Prerequisite Knowledge\n",
    "\n",
    "To effectively complete this assignment, students should be familiar with:\n",
    "- **Python Programming**: Basic syntax and data structures.\n",
    "- **PyTorch**: Understanding of tensors, autograd, and neural network modules.\n",
    "- **Deep Learning Concepts**: Knowledge of neural networks, linear transformations, activation functions, and loss functions.\n",
    "- **Basic Understanding of Natural Language Processing (NLP)**: Familiarity with sequence data and tokenization.\n",
    "- **Single-Head Attention**: Understanding of single-head attention mechanisms as a foundation for multi-head attention.\n",
    "\n",
    "#### 1.4 Mathematical Concepts and Formulas\n",
    "\n",
    "Understanding the following mathematical concepts is essential:\n",
    "\n",
    "- **Scaled Dot-Product Attention**: Calculates the attention scores by taking the dot product of queries and keys, scaling them, and applying the softmax function.\n",
    "  \n",
    "  $\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $\n",
    "  \n",
    "  - $ Q $: Queries matrix\n",
    "  - $ K $: Keys matrix\n",
    "  - $ V $: Values matrix\n",
    "  - $ d_k $: Dimension of the keys\n",
    "\n",
    "- **Multi-Head Attention**: Extends scaled dot-product attention by projecting queries, keys, and values multiple times with different linear projections.\n",
    "\n",
    "  $\n",
    "  \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O\n",
    "  $\n",
    "  \n",
    "  where each head is:\n",
    "  \n",
    "  $\n",
    "  \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "  $\n",
    "  \n",
    "  - $ W_i^Q, W_i^K, W_i^V $: Projection matrices for the $ i $-th head\n",
    "  - $ W^O $: Output projection matrix\n",
    "\n",
    "- **Softmax Function**: Converts raw attention scores into probabilities that sum to one.\n",
    "  \n",
    "  $\n",
    "  \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "  $\n",
    "\n",
    "- **Linear Transformation**: Applies linear layers to project inputs into queries, keys, and values.\n",
    "\n",
    "#### 1.5 Specific Algorithms and Techniques\n",
    "\n",
    "- **Multi-Head Attention Mechanism**: Implements multiple attention heads to capture different aspects of the input data.\n",
    "- **Linear Layers**: Used to project input embeddings into queries, keys, and values for each head.\n",
    "- **Masking**: Prevents the model from attending to certain positions, useful in tasks like language modeling and handling padding tokens.\n",
    "- **Residual Connections and Layer Normalization**: Enhances training stability and performance by allowing gradients to flow more easily and normalizing inputs to layers.\n",
    "- **Positional Encoding**: Adds information about the position of tokens in the sequence, since attention mechanisms are permutation-invariant.\n",
    "\n",
    "#### 1.6 Common Pitfalls and Misconceptions\n",
    "\n",
    "- **Dimension Mismatch**: Misaligning the dimensions of queries, keys, and values can lead to matrix multiplication errors.\n",
    "- **Ignoring Scaling Factor**: Forgetting to scale the dot product by the square root of the key dimension can cause softmax to produce extremely small gradients.\n",
    "- **Overcomplicating the Mechanism**: Multi-head attention builds upon single-head attention; adding unnecessary complexity can hinder understanding.\n",
    "- **Incorrect Masking**: Applying masks incorrectly can prevent the model from attending to necessary information or allow it to attend to irrelevant parts.\n",
    "- **Neglecting Positional Encoding**: Omitting positional information can limit the model's ability to understand the order of tokens in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tw6d3a66CzXX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.7020, Val Accuracy = 51.20%\n",
      "Epoch 2: Val Loss = 0.6807, Val Accuracy = 51.20%\n",
      "Epoch 3: Val Loss = 0.6598, Val Accuracy = 51.20%\n",
      "Epoch 4: Val Loss = 0.3814, Val Accuracy = 98.50%\n",
      "Epoch 5: Val Loss = 0.0098, Val Accuracy = 100.00%\n",
      "Epoch 6: Val Loss = 0.0009, Val Accuracy = 100.00%\n",
      "Epoch 7: Val Loss = 0.0004, Val Accuracy = 100.00%\n",
      "Epoch 8: Val Loss = 0.0003, Val Accuracy = 100.00%\n",
      "Epoch 9: Val Loss = 0.0002, Val Accuracy = 100.00%\n",
      "Epoch 10: Val Loss = 0.0002, Val Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Section 2.1: Multi-Head Attention Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # Define linear layers for queries, keys, and values\n",
    "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # Define the final linear layer\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Apply linear transformations to queries, keys, and values\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "\n",
    "        # Reshape queries, keys, and values for multi-head attention\n",
    "        queries = queries.view(N, query_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute the dot product attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        # Apply scaling\n",
    "        scaling_factor = np.sqrt(self.head_dim)\n",
    "        energy = energy / scaling_factor\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # Compute the weighted sum of values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "\n",
    "        # Concatenate the heads\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)\n",
    "\n",
    "        # Pass through the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Section 2.2: Positional Encoding Module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_length=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        positional_encoding = torch.zeros(max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        # Register as buffer to prevent it from being considered a model parameter\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :].to(x.device)\n",
    "        return x\n",
    "\n",
    "# Section 2.3: Simple Model Using Multi-Head Attention\n",
    "class SimpleMultiHeadAttentionModel(nn.Module):\n",
    "    def __init__(self, embed_size, heads, vocab_size, num_classes):\n",
    "        super(SimpleMultiHeadAttentionModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "\n",
    "        # Define an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Initialize the PositionalEncoding module\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "\n",
    "        # Initialize the MultiHeadAttention module\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "\n",
    "        # Define a fully connected output layer\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pass input through embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attention_out = self.attention(embedded, embedded, embedded, mask)\n",
    "\n",
    "        # Pool the attention output\n",
    "        pooled = attention_out.mean(dim=1)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(pooled)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Section 2.4: Training Loop (Simplified)\n",
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        # Move data to the specified device\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Section 2.5: Evaluation Function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move data to the specified device\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Section 2.6: Main Function to Initialize and Train the Model\n",
    "def main():\n",
    "    # Set hyperparameters\n",
    "    embed_size = 256\n",
    "    heads = 8\n",
    "    vocab_size = 10000  # Example vocabulary size\n",
    "    num_classes = 2     # Example number of classes\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 10\n",
    "    max_length = 100\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SimpleMultiHeadAttentionModel(embed_size, heads, vocab_size, num_classes)\n",
    "\n",
    "    # Define loss criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Prepare data loaders (use dummy data for simplicity)\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    # Example dummy data\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    inputs = torch.randint(0, vocab_size, (1000, max_length))  # (batch_size, sequence_length)\n",
    "    targets = torch.randint(0, num_classes, (1000,))\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, Val Accuracy = {val_acc*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woFewauBEDLA"
   },
   "source": [
    "## Utilizing the Transformer Class in PyTorch\n",
    "\n",
    "\n",
    "\n",
    "### Section 1: Conceptual Background\n",
    "\n",
    "#### 1.1 Theoretical Foundation and Key Concepts\n",
    "\n",
    "**Transformers** have revolutionized the field of natural language processing (NLP) and beyond by enabling models to handle sequential data efficiently without relying on recurrent structures. Introduced in the seminal paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al., Transformers leverage self-attention mechanisms to capture long-range dependencies in data.\n",
    "\n",
    "**Key Components of Transformers:**\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different parts of the input sequence when encoding a particular element.\n",
    "- **Multi-Head Attention**: Extends self-attention by allowing the model to focus on different representation subspaces simultaneously.\n",
    "- **Positional Encoding**: Injects information about the position of tokens in the sequence, compensating for the lack of recurrence.\n",
    "- **Feed-Forward Networks**: Apply non-linear transformations to the data, enhancing the model's capacity to learn complex patterns.\n",
    "- **Layer Normalization**: Stabilizes and accelerates training by normalizing the inputs across the features.\n",
    "- **Residual Connections**: Facilitate gradient flow, allowing deeper architectures without vanishing gradients.\n",
    "\n",
    "**PyTorch’s Transformer Class**: PyTorch provides a flexible and efficient implementation of the Transformer architecture through its `torch.nn.Transformer` class, enabling developers to build and train Transformer-based models with ease.\n",
    "\n",
    "#### 1.2 Real-World Applications and Relevance\n",
    "\n",
    "Transformers are at the heart of many state-of-the-art models and applications, including:\n",
    "- **Machine Translation**: Translating text from one language to another with high accuracy.\n",
    "- **Text Summarization**: Generating concise summaries of longer documents.\n",
    "- **Question Answering**: Building systems that can understand and answer questions based on given contexts.\n",
    "- **Text Generation**: Creating coherent and contextually relevant text, as seen in models like GPT-3.\n",
    "- **Speech Recognition and Generation**: Converting spoken language to text and vice versa.\n",
    "- **Image Processing**: Enhancing image recognition and generation tasks through Vision Transformers (ViT).\n",
    "\n",
    "**Example Application**: Building a **Next-Word Prediction** model using the Transformer class to assist in autocomplete functionalities, enhancing user experience in text editors and messaging apps.\n",
    "\n",
    "#### 1.3 Prerequisite Knowledge\n",
    "\n",
    "To effectively complete this assignment, students should be familiar with:\n",
    "- **Python Programming**: Basic syntax, data structures, and libraries.\n",
    "- **PyTorch**: Understanding of tensors, automatic differentiation, and neural network modules.\n",
    "- **Deep Learning Concepts**: Knowledge of neural networks, linear transformations, activation functions, and loss functions.\n",
    "- **Natural Language Processing (NLP)**: Familiarity with sequence data, tokenization, and language modeling.\n",
    "- **Attention Mechanisms**: Basic understanding of single-head and multi-head attention.\n",
    "\n",
    "#### 1.4 Mathematical Concepts and Formulas\n",
    "\n",
    "Understanding the following mathematical concepts is essential:\n",
    "\n",
    "- **Scaled Dot-Product Attention**: Calculates attention scores by taking the dot product of queries and keys, scaling them, and applying the softmax function.\n",
    "  \n",
    "  $\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $\n",
    "  \n",
    "  - $ Q $: Queries matrix\n",
    "  - $ K $: Keys matrix\n",
    "  - $ V $: Values matrix\n",
    "  - $ d_k $: Dimension of the keys\n",
    "\n",
    "- **Multi-Head Attention**: Extends scaled dot-product attention by projecting queries, keys, and values multiple times with different linear projections.\n",
    "  \n",
    "  $\n",
    "  \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O\n",
    "  $\n",
    "  \n",
    "  where each head is:\n",
    "  \n",
    "  $\n",
    "  \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "  $\n",
    "  \n",
    "  - $ W_i^Q, W_i^K, W_i^V $: Projection matrices for the $ i $-th head\n",
    "  - $ W^O $: Output projection matrix\n",
    "\n",
    "- **Positional Encoding**: Adds information about the position of tokens in the sequence using sine and cosine functions of different frequencies.\n",
    "  \n",
    "  $\n",
    "  \\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "  $\n",
    "  \n",
    "  $\n",
    "  \\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "  $\n",
    "  \n",
    "  - $ pos $: Position of the token\n",
    "  - $ i $: Dimension index\n",
    "  - $ d_{model} $: Embedding dimension\n",
    "\n",
    "#### 1.5 Specific Algorithms and Techniques\n",
    "\n",
    "- **Transformer Architecture**: Understanding the encoder-decoder structure, though for this assignment, focus will be on using the Transformer for encoding and predicting next words.\n",
    "- **Linear Layers**: Used to project input embeddings into queries, keys, and values.\n",
    "- **Masking**: Prevents the model from attending to future tokens in autoregressive tasks, ensuring causality in predictions.\n",
    "- **Layer Normalization and Residual Connections**: Enhance training stability and model performance.\n",
    "- **Positional Encoding**: Incorporates sequence order information into the model.\n",
    "\n",
    "#### 1.6 Common Pitfalls and Misconceptions\n",
    "\n",
    "- **Dimension Mismatch**: Misaligning the dimensions of queries, keys, and values can lead to matrix multiplication errors.\n",
    "- **Ignoring Scaling Factor**: Forgetting to scale the dot product by the square root of the key dimension can cause softmax to produce extremely small gradients.\n",
    "- **Incorrect Masking**: Applying masks incorrectly can prevent the model from attending to necessary information or allow it to attend to irrelevant parts.\n",
    "- **Overfitting with Small Datasets**: Using small datasets without proper regularization can lead to models that do not generalize well.\n",
    "- **Neglecting Positional Encoding**: Omitting positional information can limit the model's ability to understand the order of tokens in a sequence.\n",
    "- **Improper Learning Rate Selection**: Selecting an inappropriate learning rate can lead to poor convergence or unstable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yMv4LWxWEDcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 3.2237 | Val Loss: 3.0104 | Val Acc: 17.78%\n",
      "Epoch 2/20 | Train Loss: 2.8846 | Val Loss: 3.0277 | Val Acc: 17.78%\n",
      "Epoch 3/20 | Train Loss: 2.8465 | Val Loss: 3.0359 | Val Acc: 17.78%\n",
      "Epoch 4/20 | Train Loss: 2.8129 | Val Loss: 3.0358 | Val Acc: 17.78%\n",
      "Epoch 5/20 | Train Loss: 2.8107 | Val Loss: 3.0302 | Val Acc: 17.78%\n",
      "Epoch 6/20 | Train Loss: 2.7982 | Val Loss: 3.0233 | Val Acc: 17.78%\n",
      "Epoch 7/20 | Train Loss: 2.7967 | Val Loss: 3.0177 | Val Acc: 17.78%\n",
      "Epoch 8/20 | Train Loss: 2.7867 | Val Loss: 3.0172 | Val Acc: 17.78%\n",
      "Epoch 9/20 | Train Loss: 2.8059 | Val Loss: 3.0167 | Val Acc: 17.78%\n",
      "Epoch 10/20 | Train Loss: 2.7699 | Val Loss: 3.0161 | Val Acc: 17.78%\n",
      "Epoch 11/20 | Train Loss: 2.7873 | Val Loss: 3.0161 | Val Acc: 17.78%\n",
      "Epoch 12/20 | Train Loss: 2.7817 | Val Loss: 3.0161 | Val Acc: 17.78%\n",
      "Epoch 13/20 | Train Loss: 2.7878 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 14/20 | Train Loss: 2.7684 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 15/20 | Train Loss: 2.7625 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 16/20 | Train Loss: 2.7895 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 17/20 | Train Loss: 2.7800 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 18/20 | Train Loss: 2.8016 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 19/20 | Train Loss: 2.7760 | Val Loss: 3.0160 | Val Acc: 17.78%\n",
      "Epoch 20/20 | Train Loss: 2.7778 | Val Loss: 3.0160 | Val Acc: 17.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Section 2.1: Data Preprocessing\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, vocab):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by encoding the text and creating input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The raw text data.\n",
    "            seq_length (int): The length of each input sequence.\n",
    "            vocab (dict): A dictionary mapping characters to indices.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.encoded_text = self.encode_text(text)\n",
    "        self.inputs, self.targets = self.create_sequences()\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"\n",
    "        Encodes the text into integer indices.\n",
    "\n",
    "        Args:\n",
    "            text (str): The raw text data.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The encoded text.\n",
    "        \"\"\"\n",
    "        return [self.vocab[char] for char in text if char in self.vocab]\n",
    "\n",
    "    def create_sequences(self):\n",
    "        \"\"\"\n",
    "        Creates input and target sequences from the encoded text.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], List[int]]: Input sequences and corresponding targets.\n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(len(self.encoded_text) - self.seq_length):\n",
    "            inputs.append(self.encoded_text[i:i+self.seq_length])\n",
    "            targets.append(self.encoded_text[i+self.seq_length])\n",
    "        return inputs, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "# Section 2.2: Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_length=5000):\n",
    "        \"\"\"\n",
    "        Initializes the positional encoding module.\n",
    "\n",
    "        Args:\n",
    "            embed_size (int): The embedding dimension.\n",
    "            max_length (int): The maximum length of input sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_length, embed_size)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input embeddings of shape (batch_size, seq_length, embed_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Embeddings with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Section 2.3: Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer-based language model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embed_size (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            hidden_dim (int): Dimension of the feedforward network.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, seq_length).\n",
    "            src_mask (Tensor): Mask tensor to prevent attention to future tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.embed_size)  # Scale embeddings\n",
    "        src = self.pos_encoder(src)\n",
    "        src = self.dropout(src)\n",
    "        src = src.transpose(0, 1)  # Transformer expects (seq_length, batch_size, embed_size)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = output.transpose(0, 1)  # Back to (batch_size, seq_length, embed_size)\n",
    "        output = output.mean(dim=1)  # Aggregate over sequence length\n",
    "        out = self.fc_out(output)\n",
    "        return out\n",
    "\n",
    "# Section 2.4: Mask Generation\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    Unmasked positions are filled with float(0.0).\n",
    "\n",
    "    Args:\n",
    "        sz (int): Size of the mask (seq_length).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The generated mask of shape (sz, sz).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Section 2.5: Training Loop\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Transformer model.\n",
    "        data_loader (DataLoader): DataLoader for training data.\n",
    "        optimizer (Optimizer): Optimizer for training.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run the training on.\n",
    "        scheduler (Scheduler, optional): Learning rate scheduler.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        seq_length = inputs.size(1)\n",
    "        src_mask = generate_square_subsequent_mask(seq_length).to(device)\n",
    "        outputs = model(inputs, src_mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Section 2.6: Evaluation Function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Transformer model.\n",
    "        data_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            seq_length = inputs.size(1)\n",
    "            src_mask = generate_square_subsequent_mask(seq_length).to(device)\n",
    "            outputs = model(inputs, src_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Section 2.7: Main Function to Initialize and Train the Model\n",
    "def main():\n",
    "    # TODO: Set hyperparameters\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout = 0.1\n",
    "    seq_length = 30\n",
    "    batch_size = 64\n",
    "    num_epochs = 20\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # TODO: Load and preprocess data\n",
    "    # For simplicity, use a sample text. Replace this with a larger dataset as needed.\n",
    "    sample_text = (\n",
    "        \"In the beginning God created the heaven and the earth. \"\n",
    "        \"And the earth was without form, and void; and darkness was upon the face of the deep. \"\n",
    "        \"And the Spirit of God moved upon the face of the waters. \"\n",
    "        \"And God said, Let there be light: and there was light.\"\n",
    "    )\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocab = sorted(list(set(sample_text)))\n",
    "    stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "    itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = TextDataset(sample_text, seq_length, stoi)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # TODO: Initialize the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_size=embed_size,\n",
    "        num_heads=num_heads,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # TODO: Define loss criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # TODO: Optionally, define a learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # TODO: Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, scheduler)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # TODO: Save the trained model\n",
    "    torch.save(model.state_dict(), \"transformer_next_word_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
