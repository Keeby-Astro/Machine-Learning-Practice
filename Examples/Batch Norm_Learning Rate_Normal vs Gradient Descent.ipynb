{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Norm / Learning Rate / Normal vs Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sEg4oMwlwo6"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "In this exercise, you'll learn about normalization, a crucial technique in machine learning for standardizing input data. You'll implement normalization using two methods: manual normalization with NumPy and BatchNorm1d in PyTorch. This will help you understand the concept of normalization and how to apply it in different frameworks.\n",
        "\n",
        "Your tasks are to:\n",
        "1. Create a 2D NumPy array\n",
        "2. Perform manual normalization along the first axis\n",
        "3. Convert the NumPy array to a PyTorch tensor\n",
        "4. Apply BatchNorm1d in PyTorch\n",
        "5. Compare the results of both methods\n",
        "\n",
        "This exercise will demonstrate how normalization works and show you the convenience of using built-in PyTorch functions for common operations in deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DAeCWHnlluxo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data:\n",
            "[[0.37454012 0.95071431 0.73199394]\n",
            " [0.59865848 0.15601864 0.15599452]\n",
            " [0.05808361 0.86617615 0.60111501]\n",
            " [0.70807258 0.02058449 0.96990985]]\n",
            "\n",
            "NumPy normalized data:\n",
            "[[-0.2426183   1.09277335  0.39604745]\n",
            " [ 0.65914784 -0.82706681 -1.5497212 ]\n",
            " [-1.51591764  0.88854453 -0.04607125]\n",
            " [ 1.0993881  -1.15425107  1.199745  ]]\n",
            "\n",
            "PyTorch BatchNorm1d normalized data:\n",
            "tensor([[-0.2426,  1.0927,  0.3960],\n",
            "        [ 0.6591, -0.8270, -1.5496],\n",
            "        [-1.5158,  0.8885, -0.0461],\n",
            "        [ 1.0993, -1.1542,  1.1997]], grad_fn=<NativeBatchNormBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nThe results of the manual normalization and the PyTorch BatchNorm1d layer are different.\\nThe manual normalization is applied along the first axis (axis=0), which means that the mean and standard deviation\\nare calculated for each column. The PyTorch BatchNorm1d layer is applied along the last dimension (axis=-1),\\nwhich means that the mean and standard deviation are calculated for each row. This is why the results are different.\\nTo get the same results, we need to transpose the tensor before applying the BatchNorm1d layer.\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a 2D NumPy array with shape (4, 3) and random values\n",
        "data = np.random.rand(4, 3)\n",
        "\n",
        "# Perform manual normalization along the first axis (axis=0)\n",
        "# Use np.mean() and np.std()\n",
        "normalized_numpy = np.zeros_like(data)\n",
        "for i in range(data.shape[1]):\n",
        "    mean = np.mean(data[:, i])\n",
        "    std = np.std(data[:, i])\n",
        "    normalized_numpy[:, i] = (data[:, i] - mean) / std\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor\n",
        "tensor_data = torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "# Create a BatchNorm1d layer and apply it to the tensor\n",
        "batch_norm = nn.BatchNorm1d(num_features=3)\n",
        "normalized_torch = batch_norm(tensor_data)\n",
        "\n",
        "# Print and compare results\n",
        "print(\"Original data:\")\n",
        "print(data)\n",
        "\n",
        "print(\"\\nNumPy normalized data:\")\n",
        "print(normalized_numpy)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm1d normalized data:\")\n",
        "print(normalized_torch)\n",
        "\n",
        "# Compare the results and explain any differences you observe\n",
        "\n",
        "'''\n",
        "The results of the manual normalization and the PyTorch BatchNorm1d layer are different.\n",
        "The manual normalization is applied along the first axis (axis=0), which means that the mean and standard deviation\n",
        "are calculated for each column. The PyTorch BatchNorm1d layer is applied along the last dimension (axis=-1),\n",
        "which means that the mean and standard deviation are calculated for each row. This is why the results are different.\n",
        "To get the same results, we need to transpose the tensor before applying the BatchNorm1d layer.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ucRE1IngVZ"
      },
      "source": [
        "## Learning Rate\n",
        "In this exercise, you'll explore the concept of learning rate in optimization algorithms, which is crucial in training machine learning models. The learning rate determines the step size at each iteration while moving toward a minimum of the loss function.\n",
        "\n",
        "You'll complete two tasks:\n",
        "\n",
        "1. Implement a simple gradient descent algorithm in pure Python to see how different learning rates affect the optimization process.\n",
        "2. Use PyTorch to train a simple linear model and observe how changing the learning rate impacts the training process.\n",
        "\n",
        "This exercise will help you understand why choosing an appropriate learning rate is important and how it affects model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y8RnO9VDng6o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Part 1: Simple Gradient Descent ---\n",
            "Iteration 0: x = 4.0000, f(x) = 16.0000\n",
            "Iteration 5: x = 1.3107, f(x) = 1.7180\n",
            "Iteration 10: x = 0.4295, f(x) = 0.1845\n",
            "Iteration 15: x = 0.1407, f(x) = 0.0198\n",
            "Iteration 20: x = 0.0461, f(x) = 0.0021\n",
            "Learning rate 0.1: x = 0.018889465931478583\n",
            "\n",
            "Iteration 0: x = 4.9000, f(x) = 24.0100\n",
            "Iteration 5: x = 4.4292, f(x) = 19.6179\n",
            "Iteration 10: x = 4.0037, f(x) = 16.0293\n",
            "Iteration 15: x = 3.6190, f(x) = 13.0971\n",
            "Iteration 20: x = 3.2713, f(x) = 10.7013\n",
            "Learning rate 0.01: x = 3.0173236488944846\n",
            "\n",
            "Iteration 0: x = 0.0000, f(x) = 0.0000\n",
            "Iteration 5: x = 0.0000, f(x) = 0.0000\n",
            "Iteration 10: x = 0.0000, f(x) = 0.0000\n",
            "Iteration 15: x = 0.0000, f(x) = 0.0000\n",
            "Iteration 20: x = 0.0000, f(x) = 0.0000\n",
            "Learning rate 0.5: x = 0.0\n",
            "\n",
            "\n",
            "--- Part 2: PyTorch Implementation ---\n",
            "Epoch 0, Loss: 145.8912\n",
            "Epoch 10, Loss: 0.9808\n",
            "Epoch 20, Loss: 0.9518\n",
            "Epoch 30, Loss: 0.9254\n",
            "Epoch 40, Loss: 0.9015\n",
            "Epoch 50, Loss: 0.8797\n",
            "Epoch 60, Loss: 0.8599\n",
            "Epoch 70, Loss: 0.8420\n",
            "Epoch 80, Loss: 0.8256\n",
            "Epoch 90, Loss: 0.8108\n",
            "\n",
            "Epoch 0, Loss: 0.7973\n",
            "Epoch 10, Loss: 4676.5410\n",
            "Epoch 20, Loss: 32067337987947495424.0000\n",
            "Epoch 30, Loss: 219921474085973062593906102184706048.0000\n",
            "Epoch 40, Loss: inf\n",
            "Epoch 50, Loss: inf\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 70, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 90, Loss: nan\n",
            "\n",
            "Epoch 0, Loss: nan\n",
            "Epoch 10, Loss: nan\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 30, Loss: nan\n",
            "Epoch 40, Loss: nan\n",
            "Epoch 50, Loss: nan\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 70, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 90, Loss: nan\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Part 1: Simple gradient descent in pure Python\n",
        "\n",
        "print(\"\\n--- Part 1: Simple Gradient Descent ---\")\n",
        "\n",
        "# Define a simple quadratic function f(x) = x^2 and its derivative df(x) = 2x\n",
        "def f(x):\n",
        "    \"\"\"A simple quadratic function f(x) = x^2\"\"\"\n",
        "    return x**2\n",
        "\n",
        "def df(x):\n",
        "    \"\"\"Derivative of f(x)\"\"\"\n",
        "    return 2*x\n",
        "\n",
        "# Implement the gradient descent algorithm\n",
        "def gradient_descent(start, learn_rate, num_iterations):\n",
        "    x = start\n",
        "    for i in range(num_iter):\n",
        "        # Implement the gradient descent update rule\n",
        "        x = x - learn_rate * df(x)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Iteration {i}: x = {x:.4f}, f(x) = {f(x):.4f}\")\n",
        "    return x\n",
        "\n",
        "# Run gradient descent with different learning rates\n",
        "# Use start=5.0, num_iterations=25, and learning rates: 0.1, 0.01, 0.5\n",
        "start = 5.0\n",
        "num_iter = 25\n",
        "learning_rate_01 = 0.1\n",
        "result = gradient_descent(start, learning_rate_01, num_iter)\n",
        "print(\"Learning rate 0.1: x =\", result)\n",
        "print(\"\")\n",
        "\n",
        "learning_rate_001 = 0.01\n",
        "result = gradient_descent(start, learning_rate_001, num_iter)\n",
        "print(\"Learning rate 0.01: x =\", result)\n",
        "print(\"\")\n",
        "\n",
        "learning_rate_05 = 0.5\n",
        "result = gradient_descent(start, learning_rate_05, num_iter)\n",
        "print(\"Learning rate 0.5: x =\", result)\n",
        "print(\"\")\n",
        "# Print the results and compare them\n",
        "\n",
        "print(\"\\n--- Part 2: PyTorch Implementation ---\")\n",
        "\n",
        "# Generate some sample data\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand(100, 1) * 10\n",
        "y = 2 * X + 1 + torch.randn(100, 1)\n",
        "\n",
        "# Define a simple linear model\n",
        "class SimpleLinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Create an instance of the model, define loss function and optimizer\n",
        "model = SimpleLinearModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "def train(learning_rate, num_epochs):\n",
        "    # Update the optimizer with the new learning rate\n",
        "    optimizer.param_groups[0]['lr'] = learning_rate\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Implement the training loop\n",
        "        # 1. Compute the model output\n",
        "        # 2. Compute the loss\n",
        "        # 3. Backpropagate the loss\n",
        "        # 4. Update the model parameters\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X) # Compute the model output\n",
        "        loss = criterion(y_pred, y) # Compute the loss\n",
        "        loss.backward() # Backpropagation the loss\n",
        "        optimizer.step() # Update the model parameters\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Train the model with different learning rates (e.g., 0.01, 0.1, 0.5)\n",
        "# Print the results and compare them\n",
        "learning_rate_001 = 0.01\n",
        "train(learning_rate_001, 100)\n",
        "print(\"\")\n",
        "\n",
        "learning_rate_01 = 0.1\n",
        "train(learning_rate_01, 100)\n",
        "print(\"\")\n",
        "\n",
        "learning_rate_05 = 0.5\n",
        "train(learning_rate_05, 100)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRuGBztFpKyS"
      },
      "source": [
        "## Analytical Solution vs. Iterative Solution\n",
        "In this exercise, you'll explore two methods to solve the linear equation X*theta=Y, where X is the input matrix, Y is the target vector, and theta is the parameter vector we want to determine. This is a fundamental problem in linear regression and many other machine learning tasks.\n",
        "\n",
        "You'll implement two approaches in PyTorch:\n",
        "\n",
        "1. Analytical solution: theta = (X^T * X)^(-1) * (X^T * Y)\n",
        "2. Iterative solution using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O8J6sgdapJ6S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analytical solution theta: tensor([ 1.4982, -0.8054,  2.0325])\n",
            "\n",
            "Iteration 0, Loss: 2.2463\n",
            "Iteration 1000, Loss: 0.0089\n",
            "Iteration 2000, Loss: 0.0089\n",
            "Iteration 3000, Loss: 0.0089\n",
            "Iteration 4000, Loss: 0.0089\n",
            "Iteration 5000, Loss: 0.0089\n",
            "Iteration 6000, Loss: 0.0089\n",
            "Iteration 7000, Loss: 0.0089\n",
            "Iteration 8000, Loss: 0.0089\n",
            "Iteration 9000, Loss: 0.0089\n",
            "Gradient descent solution theta: tensor([ 1.4982, -0.8054,  2.0325])\n",
            "True theta: tensor([ 1.5000, -0.8000,  2.0000])\n",
            "\n",
            "Analytical solution loss: 0.008850558660924435\n",
            "Gradient descent solution loss: 0.008850560523569584\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Generate some sample data\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand(100, 3)\n",
        "true_theta = torch.tensor([1.5, -0.8, 2.0])\n",
        "Y = torch.matmul(X, true_theta) + torch.randn(100) * 0.1\n",
        "\n",
        "# Implement the analytical solution\n",
        "def analytical_solution(X, Y):\n",
        "    # Hint: Use torch.matmul() for matrix multiplication and torch.inverse() for matrix inversion\n",
        "    XTX = torch.matmul(X.T, X)\n",
        "    XTY = torch.matmul(X.T, Y)\n",
        "    theta = torch.matmul(torch.inverse(XTX), XTY)\n",
        "    return theta\n",
        "\n",
        "# Implement the gradient descent solution\n",
        "def gradient_descent_solution(X, Y, learning_rate, num_iterations):\n",
        "    theta = torch.randn(3)  # Initialize theta with random values\n",
        "    for i in range(num_iterations):\n",
        "        # Implement the gradient descent update rule\n",
        "        # 1. Compute the predicted Y\n",
        "        # 2. Compute the loss (mean squared error)\n",
        "        # 3. Compute the gradient\n",
        "        # 4. Update theta\n",
        "\n",
        "        Y_pred = torch.matmul(X, theta)\n",
        "        loss = torch.mean(torch.square(Y_pred - Y))\n",
        "        gradient = 2 * torch.matmul(X.T, Y_pred - Y)\n",
        "        theta = theta - learning_rate * gradient\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Call both functions and compare the results\n",
        "analytical_theta = analytical_solution(X, Y)\n",
        "print(\"Analytical solution theta:\", analytical_theta)\n",
        "print(\"\")\n",
        "\n",
        "gradient_descent_theta = gradient_descent_solution(X, Y, 0.01, 10000)\n",
        "print(\"\")\n",
        "print(\"Gradient descent solution theta:\", gradient_descent_theta)\n",
        "\n",
        "print(\"True theta:\", true_theta)\n",
        "print(\"\")\n",
        "\n",
        "# Compare the mean squared error for both solutions\n",
        "# Hint: Use torch.mean() and torch.square()\n",
        "\n",
        "analytical_Y_pred = torch.matmul(X, analytical_theta)\n",
        "analytical_loss = torch.mean(torch.square(analytical_Y_pred - Y))\n",
        "print(\"Analytical solution loss:\", analytical_loss.item())\n",
        "\n",
        "gradient_descent_Y_pred = torch.matmul(X, gradient_descent_theta)\n",
        "gradient_descent_loss = torch.mean(torch.square(gradient_descent_Y_pred - Y))\n",
        "print(\"Gradient descent solution loss:\", gradient_descent_loss.item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
